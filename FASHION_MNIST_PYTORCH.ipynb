{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "FASHION_MNIST_PYTORCH.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMXK7HaJk1pLb9xKRR2KlyY",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/wakharemadhukar/projects/blob/master/FASHION_MNIST_PYTORCH.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c21YX0xt3-jk"
      },
      "source": [
        "import numpy as np\r\n",
        "import pandas as pd\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "\r\n",
        "import torch\r\n",
        "import torch.nn as nn\r\n",
        "from torch.autograd import Variable\r\n",
        "\r\n",
        "import torchvision\r\n",
        "import torchvision.transforms as transforms\r\n",
        "from torch.utils.data import Dataset, DataLoader\r\n",
        "from sklearn.metrics import confusion_matrix"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uud7DPExDKJ0"
      },
      "source": [
        "class FashionDataset(Dataset):\r\n",
        "    \"\"\"User defined class to build a datset using Pytorch class Dataset.\"\"\"\r\n",
        "    \r\n",
        "    def __init__(self, data, transform = None):\r\n",
        "        \"\"\"Method to initilaize variables.\"\"\" \r\n",
        "        self.fashion_MNIST = list(data.values)\r\n",
        "        self.transform = transform\r\n",
        "        \r\n",
        "        label = []\r\n",
        "        image = []\r\n",
        "        \r\n",
        "        for i in self.fashion_MNIST:\r\n",
        "             # first column is of labels.\r\n",
        "            label.append(i[0])\r\n",
        "            image.append(i[1:])\r\n",
        "        self.labels = np.asarray(label)\r\n",
        "        # Dimension of Images = 28 * 28 * 1. where height = width = 28 and color_channels = 1.\r\n",
        "        self.images = np.asarray(image).reshape(-1, 28, 28, 1).astype('float32')\r\n",
        "\r\n",
        "    def __getitem__(self, index):\r\n",
        "        label = self.labels[index]\r\n",
        "        image = self.images[index]\r\n",
        "        \r\n",
        "        if self.transform is not None:\r\n",
        "            image = self.transform(image)\r\n",
        "\r\n",
        "        return image, label\r\n",
        "\r\n",
        "    def __len__(self):\r\n",
        "        return len(self.images)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jp5uJEZMDdE6"
      },
      "source": [
        "train_set = torchvision.datasets.FashionMNIST(\"./data\", download=True, transform=\r\n",
        "                                                transforms.Compose([transforms.ToTensor()]))\r\n",
        "test_set = torchvision.datasets.FashionMNIST(\"./data\", download=True, train=False, transform=\r\n",
        "                                               transforms.Compose([transforms.ToTensor()]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p8RWuArvDmSD"
      },
      "source": [
        "train_loader = torch.utils.data.DataLoader(train_set, \r\n",
        "                                           batch_size=100)\r\n",
        "test_loader = torch.utils.data.DataLoader(test_set,\r\n",
        "                                          batch_size=100)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eblfU67QDp1w"
      },
      "source": [
        "def output_label(label):\r\n",
        "    output_mapping = {\r\n",
        "                 0: \"T-shirt/Top\",\r\n",
        "                 1: \"Trouser\",\r\n",
        "                 2: \"Pullover\",\r\n",
        "                 3: \"Dress\",\r\n",
        "                 4: \"Coat\", \r\n",
        "                 5: \"Sandal\", \r\n",
        "                 6: \"Shirt\",\r\n",
        "                 7: \"Sneaker\",\r\n",
        "                 8: \"Bag\",\r\n",
        "                 9: \"Ankle Boot\"\r\n",
        "                 }\r\n",
        "    input = (label.item() if type(label) == torch.Tensor else label)\r\n",
        "    return output_mapping[input]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xPLRw8k5DvFR",
        "outputId": "1beaaaa1-fb48-4ec6-9e80-ae89922c5fe6"
      },
      "source": [
        "a = next(iter(train_loader))\r\n",
        "a[0].size()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([100, 1, 28, 28])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yCCwaPgND0VD",
        "outputId": "af0b880a-7b1d-4d5b-a6c9-cb05f842429d"
      },
      "source": [
        "len(train_set)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "60000"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "id": "_L4PeEZUD8ZU",
        "outputId": "7acd91f2-139c-423f-e8ff-617ad3ccfd21"
      },
      "source": [
        "image, label = next(iter(train_set))\r\n",
        "plt.imshow(image.squeeze(), cmap=\"gray\")\r\n",
        "print(label)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "9\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Sb-iZ2DEAKL",
        "outputId": "9ace9d0b-1b90-4c05-9767-85ab83d7d3f4"
      },
      "source": [
        "\r\n",
        "demo_loader = torch.utils.data.DataLoader(train_set, batch_size=10)\r\n",
        "\r\n",
        "batch = next(iter(demo_loader))\r\n",
        "images, labels = batch\r\n",
        "print(type(images), type(labels))\r\n",
        "print(images.shape, labels.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'torch.Tensor'> <class 'torch.Tensor'>\n",
            "torch.Size([10, 1, 28, 28]) torch.Size([10])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5g_1VsnWEq7L"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qw5dgKgYE6XB"
      },
      "source": [
        "## Building a CNN \r\n",
        "\r\n",
        "\r\n",
        "*   Make a model class (FashionCNN in our case)\r\n",
        "    * It inherit nn.Module class that is a super class for all the neural networks in Pytorch.\r\n",
        "* Our Neural Net has following layers:\r\n",
        "    * Two Sequential layers each consists of following layers-\r\n",
        "        * Convolution layer that has kernel size of 3 * 3, padding = 1 (zero_padding) in 1st layer and padding = 0 in second one. Stride of 1 in both layer.\r\n",
        "        * Batch Normalization layer.\r\n",
        "        * Acitvation function: ReLU.\r\n",
        "        * Max Pooling layer with kernel size of 2 * 2 and stride 2.\r\n",
        "     * Flatten out the output for dense layer(a.k.a. fully connected layer).\r\n",
        "     * 3 Fully connected layer  with different in/out features.\r\n",
        "     * 1 Dropout layer that has class probability p = 0.25.\r\n",
        "  \r\n",
        "     * All the functionaltiy is given in forward method that defines the forward pass of CNN.\r\n",
        "     * Our input image is changing in a following way:\r\n",
        "        * First Convulation layer : input: 28 \\* 28 \\* 3, output: 28 \\* 28 \\* 32\r\n",
        "        * First Max Pooling layer : input: 28 \\* 28 \\* 32, output: 14 \\* 14 \\* 32\r\n",
        "        * Second Conv layer : input : 14 \\* 14 \\* 32, output: 12 \\* 12 \\* 64\r\n",
        "        * Second Max Pooling layer : 12 \\* 12 \\* 64, output:  6 \\* 6 \\* 64\r\n",
        "    * Final fully connected layer has 10 output features for 10 types of clothes.\r\n",
        "\r\n",
        "> Lets implementing the network...\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZQSKSaqhE6_M"
      },
      "source": [
        "class FashionCNN(nn.Module):\r\n",
        "    \r\n",
        "    def __init__(self):\r\n",
        "        super(FashionCNN, self).__init__()\r\n",
        "        \r\n",
        "        self.layer1 = nn.Sequential(\r\n",
        "            nn.Conv2d(in_channels=1, out_channels=32, kernel_size=3, padding=1),\r\n",
        "            nn.BatchNorm2d(32),\r\n",
        "            nn.ReLU(),\r\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2)\r\n",
        "        )\r\n",
        "        \r\n",
        "        self.layer2 = nn.Sequential(\r\n",
        "            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3),\r\n",
        "            nn.BatchNorm2d(64),\r\n",
        "            nn.ReLU(),\r\n",
        "            nn.MaxPool2d(2)\r\n",
        "        )\r\n",
        "        \r\n",
        "        self.fc1 = nn.Linear(in_features=64*6*6, out_features=600)\r\n",
        "        self.drop = nn.Dropout2d(0.25)\r\n",
        "        self.fc2 = nn.Linear(in_features=600, out_features=120)\r\n",
        "        self.fc3 = nn.Linear(in_features=120, out_features=10)\r\n",
        "        \r\n",
        "    def forward(self, x):\r\n",
        "        out = self.layer1(x)\r\n",
        "        out = self.layer2(out)\r\n",
        "        out = out.view(out.size(0), -1)\r\n",
        "        out = self.fc1(out)\r\n",
        "        out = self.drop(out)\r\n",
        "        out = self.fc2(out)\r\n",
        "        out = self.fc3(out)\r\n",
        "        \r\n",
        "        return out\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XW4FeMACFChH",
        "outputId": "5847490e-d3a2-4953-8581-a7fa104c3b80"
      },
      "source": [
        "model = FashionCNN()\r\n",
        "model.to(device)\r\n",
        "\r\n",
        "error = nn.CrossEntropyLoss()\r\n",
        "\r\n",
        "learning_rate = 0.001\r\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\r\n",
        "print(model)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "FashionCNN(\n",
            "  (layer1): Sequential(\n",
            "    (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (2): ReLU()\n",
            "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  )\n",
            "  (layer2): Sequential(\n",
            "    (0): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1))\n",
            "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (2): ReLU()\n",
            "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  )\n",
            "  (fc1): Linear(in_features=2304, out_features=600, bias=True)\n",
            "  (drop): Dropout2d(p=0.25, inplace=False)\n",
            "  (fc2): Linear(in_features=600, out_features=120, bias=True)\n",
            "  (fc3): Linear(in_features=120, out_features=10, bias=True)\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "97CcjxweFJl0",
        "outputId": "feb1fccb-cf03-4d6a-8e21-829ac638a7d7"
      },
      "source": [
        "## training and Testing\r\n",
        "num_epochs = 5\r\n",
        "count = 0\r\n",
        "# Lists for visualization of loss and accuracy \r\n",
        "loss_list = []\r\n",
        "iteration_list = []\r\n",
        "accuracy_list = []\r\n",
        "\r\n",
        "# Lists for knowing classwise accuracy\r\n",
        "predictions_list = []\r\n",
        "labels_list = []\r\n",
        "\r\n",
        "for epoch in range(num_epochs):\r\n",
        "    for images, labels in train_loader:\r\n",
        "        # Transfering images and labels to GPU if available\r\n",
        "        images, labels = images.to(device), labels.to(device)\r\n",
        "    \r\n",
        "        train = Variable(images.view(100, 1, 28, 28))\r\n",
        "        labels = Variable(labels)\r\n",
        "        \r\n",
        "        # Forward pass \r\n",
        "        outputs = model(train)\r\n",
        "        loss = error(outputs, labels)\r\n",
        "        \r\n",
        "        # Initializing a gradient as 0 so there is no mixing of gradient among the batches\r\n",
        "        optimizer.zero_grad()\r\n",
        "        \r\n",
        "        #Propagating the error backward\r\n",
        "        loss.backward()\r\n",
        "        \r\n",
        "        # Optimizing the parameters\r\n",
        "        optimizer.step()\r\n",
        "    \r\n",
        "        count += 1\r\n",
        "    \r\n",
        "    # Testing the model\r\n",
        "    \r\n",
        "        if not (count % 50):    # It's same as \"if count % 50 == 0\"\r\n",
        "            total = 0\r\n",
        "            correct = 0\r\n",
        "        \r\n",
        "            for images, labels in test_loader:\r\n",
        "                images, labels = images.to(device), labels.to(device)\r\n",
        "                labels_list.append(labels)\r\n",
        "            \r\n",
        "                test = Variable(images.view(100, 1, 28, 28))\r\n",
        "            \r\n",
        "                outputs = model(test)\r\n",
        "            \r\n",
        "                predictions = torch.max(outputs, 1)[1].to(device)\r\n",
        "                predictions_list.append(predictions)\r\n",
        "                correct += (predictions == labels).sum()\r\n",
        "            \r\n",
        "                total += len(labels)\r\n",
        "            \r\n",
        "            accuracy = correct * 100 / total\r\n",
        "            loss_list.append(loss.data)\r\n",
        "            iteration_list.append(count)\r\n",
        "            accuracy_list.append(accuracy)\r\n",
        "        \r\n",
        "        if not (count % 500):\r\n",
        "            print(\"Iteration: {}, Loss: {}, Accuracy: {}%\".format(count, loss.data, accuracy))\r\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Iteration: 500, Loss: 0.4601171612739563, Accuracy: 87.37000274658203%\n",
            "Iteration: 1000, Loss: 0.3674651086330414, Accuracy: 87.41999816894531%\n",
            "Iteration: 1500, Loss: 0.2921752631664276, Accuracy: 88.58999633789062%\n",
            "Iteration: 2000, Loss: 0.18199115991592407, Accuracy: 89.06999969482422%\n",
            "Iteration: 2500, Loss: 0.14259761571884155, Accuracy: 89.70999908447266%\n",
            "Iteration: 3000, Loss: 0.15648917853832245, Accuracy: 90.0199966430664%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XdIKndy9FUC7"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OGjTTwSxEkPZ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}